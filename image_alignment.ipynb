{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image alignment\n",
    "\n",
    "The purpose of this notebook is to resize and align the ppl, xpl, and false-color TIMA images for use in data collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test out manual pixel coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two lists of matched coordinate pairs\n",
    "\n",
    "#xpl\n",
    "pts1 = np.array([[126, 124], [144, 153], [188, 175], [83, 120], [10,49],\n",
    "                 [138, 71],[52, 168], [28, 182], [124, 25], [79,20]])\n",
    "\n",
    "#ppl\n",
    "pts2 = np.array([[125, 74], [152, 56], [175, 11], [120, 116], [50, 190],\n",
    "                 [71, 61],[168, 147], [182, 170], [25, 77], [19, 120]])\n",
    "# load and view sample image\n",
    "img = cv2.imread('Images/xplimg.png')\n",
    "template = cv2.imread('Images/template.jpg')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(template)\n",
    "# plot dots\n",
    "plt.scatter(pts2[:,0], pts2[:,1], color='r')\n",
    "plt.title('Template Image')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img)\n",
    "# plot red dot\n",
    "plt.scatter(pts1[:,0], pts1[:,1],color='r')\n",
    "plt.title('XPL Image')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(H, mask) = cv2.findHomography(pts1, pts2, method = cv2.RANSAC)\n",
    "\n",
    "# use the homography matrix to align the images\n",
    "(h, w) = template.shape[:2]\n",
    "aligned = cv2.warpPerspective(img, H, (w,h))\n",
    "\n",
    "# resize both the aligned and template images so we can easily \n",
    "# visualize them on the screen\n",
    "aligned = imutils.resize(aligned, width=700)\n",
    "template = imutils.resize(template, width=700)\n",
    "\n",
    "# side-by-side comparison of the output aligned image and the template\n",
    "stacked = np.hstack([aligned, template])\n",
    "\n",
    "# second image alignment visualization will be overlaying the\n",
    "# aligned image on the template to get an idea of how good\n",
    "# the image alignment is\n",
    "\n",
    "overlay = template.copy()\n",
    "output = aligned.copy()\n",
    "cv2.addWeighted(overlay, 0.5, output, 0.5, 0, output)\n",
    "\n",
    "# show the two output inmage alignment visualizations\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.title('Stacked image')\n",
    "plt.imshow(stacked)\n",
    "plt.subplot(2,1,2)\n",
    "plt.title('Overlayed image')\n",
    "plt.imshow(output)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop way to georeference both images and manually select keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "import numpy as np\n",
    "from widget import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl = Image.open('/Users/kacikus/Dropbox/AutomatedMineralogy_Project/Automated-Mineralogy/Images/ppltest.jpg')\n",
    "xpl = Image.open('/Users/kacikus/Dropbox/AutomatedMineralogy_Project/Automated-Mineralogy/Images/xpltest.png')\n",
    "labels = Image.open('/Users/kacikus/Dropbox/AutomatedMineralogy_Project/Automated-Mineralogy/Images/labels.png')\n",
    "\n",
    "list1, list2, list3 = main(ppl, xpl, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1"
   ]
  },
  {
   "source": [
    "# Re-write to be formated as classes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference:\n",
    "https://www.programmersought.com/article/38705290853/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2\n",
    "The above method did not work very well and warped the image quite a bit.\n",
    "\n",
    "I need to:\n",
    "1. Transform an image to fit perfectly on top on another image\n",
    "2. crop so they have the same pixel positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('Images/ppltest.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((25,25), np.float32)/625\n",
    "avg = cv2.filter2D(img, -1, kernel)\n",
    "blur = cv2.GaussianBlur(img, (25,25),0)\n",
    "\n",
    "plt.figure(figsize=(15,20))\n",
    "plt.subplot(131),plt.imshow(img),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(132),plt.imshow(dst),plt.title('Averaging')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(133),plt.imshow(blur),plt.title('Gaussian Blur')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('Images/image.jpg')\n",
    "edges = cv2.Canny(img, 100, 100)\n",
    "\n",
    "plt.subplot(121),plt.imshow(img,cmap = 'gray')\n",
    "plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(edges,cmap = 'gray')\n",
    "plt.title('Edge Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('Images/xplimg.png')\n",
    "edges = cv2.Canny(img, 50, 2)\n",
    "\n",
    "plt.subplot(121),plt.imshow(img,cmap = 'gray')\n",
    "plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(edges,cmap = 'gray')\n",
    "plt.title('Edge Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare image matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open images\n",
    "# transformed ppl image\n",
    "#img = cv2.imread('Images/image.jpg')\n",
    "\n",
    "# larger ppl image\n",
    "#img = cv2.imread('Images/larger_img.jpg')\n",
    "\n",
    "# transformed xpl image\n",
    "img = cv2.imread('Images/xplimg.png')\n",
    "imgGray = cv2.Canny(img, 50, 2)\n",
    "\n",
    "# original ppl image\n",
    "template = cv2.imread('Images/template.jpg')\n",
    "templateGray = cv2.Canny(template, 100, 100)\n",
    "\n",
    "'''Uncomment when you arent using edges'''\n",
    "# img = cv2.imread('smileyface.jpeg')\n",
    "# template = cv2.imread('template.jpeg')\n",
    "\n",
    "# convert to grayscale\n",
    "#imageGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#templateGray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)\n",
    "height, width = templateGray.shape\n",
    "\n",
    "# use ORB to detect keypoints and extract (binary) \n",
    "# local invariant features\n",
    "orb = cv2.ORB_create(500)\n",
    "kp1, d1 = orb.detectAndCompute(imageGray, None)\n",
    "kp2, d2 = orb.detectAndCompute(templateGray, None)\n",
    "\n",
    "# match features\n",
    "method = cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING\n",
    "matcher = cv2.DescriptorMatcher_create(method)\n",
    "matches = matcher.match(d1, d2, None)\n",
    "\n",
    "# sort matches by their distance (smaller distance,\n",
    "# means more similar features)\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "no_of_matches = len(matches)\n",
    "\n",
    "# keep only the top matches\n",
    "keep = int(len(matches) * 0.05)\n",
    "matches = matches[:keep]\n",
    "\n",
    "# check to see if we should visualize the matched keypoints\n",
    "matchedVis = cv2.drawMatches(img, kp1, template, kp2, matches, None)\n",
    "matchedVis = imutils.resize(matchedVis, width = 1000)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(matchedVis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allocate memory for the keypoints (x, y)-coordinates from the\n",
    "# top matches -- we'll use these coordinates to compute homography matrix\n",
    "pts1 = np.zeros((len(matches), 2), dtype='float')\n",
    "pts2 = np.zeros((len(matches), 2), dtype='float')\n",
    "\n",
    "# loop over the top matches\n",
    "for (i, m) in enumerate(matches):\n",
    "    # indicate that the two keypoints int he respective images\n",
    "    # map to each other\n",
    "    pts1[i] = kp1[m.queryIdx].pt\n",
    "    pts2[i] = kp2[m.trainIdx].pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our organized pairs of keypoint matches, now we are ready to align the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute homography matrix between two sets of matched points\n",
    "(H, mask) = cv2.findHomography(pts1, pts2, method = cv2.RANSAC)\n",
    "\n",
    "# use the homography matrix to align the images\n",
    "(h, w) = template.shape[:2]\n",
    "aligned = cv2.warpPerspective(img, H, (w,h))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,10))\n",
    "# plot original (left) and template (right)\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Original (left); template (right)')\n",
    "plt.imshow(matchedVis)\n",
    "# plot the newly aligned image\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Aligned image')\n",
    "plt.imshow(aligned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare both the newly aligned image and the old image stacked on top of each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize both the aligned and template images so we can easily \n",
    "# visualize them on the screen\n",
    "aligned = imutils.resize(aligned, width=700)\n",
    "template = imutils.resize(template, width=700)\n",
    "\n",
    "# side-by-side comparison of the output aligned image and the template\n",
    "stacked = np.hstack([aligned, template])\n",
    "\n",
    "# second image alignment visualization will be overlaying the\n",
    "# aligned image on the template to get an idea of how good\n",
    "# the image alignment is\n",
    "\n",
    "overlay = template.copy()\n",
    "output = aligned.copy()\n",
    "cv2.addWeighted(overlay, 0.5, output, 0.5, 0, output)\n",
    "\n",
    "# show the two output inmage alignment visualizations\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.title('Stacked image')\n",
    "plt.imshow(stacked)\n",
    "plt.subplot(2,1,2)\n",
    "plt.title('Overlayed image')\n",
    "plt.imshow(output)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://www.geeksforgeeks.org/image-registration-using-opencv-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}